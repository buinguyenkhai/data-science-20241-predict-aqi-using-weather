{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c683be19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T16:57:14.704865Z",
     "iopub.status.busy": "2024-12-17T16:57:14.704526Z",
     "iopub.status.idle": "2024-12-17T16:57:21.181854Z",
     "shell.execute_reply": "2024-12-17T16:57:21.180970Z"
    },
    "papermill": {
     "duration": 6.484293,
     "end_time": "2024-12-17T16:57:21.183726",
     "exception": false,
     "start_time": "2024-12-17T16:57:14.699433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import joblib\n",
    "#from convlstm import ConvLSTM\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "#wandb.login(key='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4eaaa692",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T16:57:21.192698Z",
     "iopub.status.busy": "2024-12-17T16:57:21.192399Z",
     "iopub.status.idle": "2024-12-17T16:57:21.211275Z",
     "shell.execute_reply": "2024-12-17T16:57:21.210386Z"
    },
    "papermill": {
     "duration": 0.025512,
     "end_time": "2024-12-17T16:57:21.213004",
     "exception": false,
     "start_time": "2024-12-17T16:57:21.187492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ConvLSTM architecture. All credits goes to https://github.com/ndrplz/ConvLSTM_pytorch \n",
    "\"\"\"\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
    "        \"\"\"\n",
    "        Initialize ConvLSTM cell.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_dim: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: (int, int)\n",
    "            Size of the convolutional kernel.\n",
    "        bias: bool\n",
    "            Whether or not to add the bias.\n",
    "        \"\"\"\n",
    "\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              #padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n",
    "\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))\n",
    "\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters:\n",
    "        input_dim: Number of channels in input\n",
    "        hidden_dim: Number of hidden channels\n",
    "        kernel_size: Size of kernel in convolutions\n",
    "        num_layers: Number of LSTM layers stacked on each other\n",
    "        batch_first: Whether or not dimension 0 is the batch or not\n",
    "        bias: Bias or no bias in Convolution\n",
    "        return_all_layers: Return the list of computations for all layers\n",
    "        Note: Will do same padding.\n",
    "\n",
    "    Input:\n",
    "        A tensor of size B, T, C, H, W or T, B, C, H, W\n",
    "    Output:\n",
    "        A tuple of two lists of length num_layers (or length 1 if return_all_layers is False).\n",
    "            0 - layer_output_list is the list of lists of length T of each output\n",
    "            1 - last_state_list is the list of last states\n",
    "                    each element of the list is a tuple (h, c) for hidden state and memory\n",
    "    Example:\n",
    "        >> x = torch.rand((32, 10, 64, 128, 128))\n",
    "        >> convlstm = ConvLSTM(64, 16, 3, 1, True, True, False)\n",
    "        >> _, last_states = convlstm(x)\n",
    "        >> h = last_states[0][0]  # 0 for layer index, 0 for h index\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,\n",
    "                 batch_first=False, bias=True, return_all_layers=False):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self._check_kernel_size_consistency(kernel_size)\n",
    "\n",
    "        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
    "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n",
    "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
    "            raise ValueError('Inconsistent list length.')\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "\n",
    "        cell_list = []\n",
    "        for i in range(0, self.num_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
    "\n",
    "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias))\n",
    "\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tensor: todo\n",
    "            5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n",
    "        hidden_state: todo\n",
    "            None. todo implement stateful\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        last_state_list, layer_output\n",
    "        \"\"\"\n",
    "        if not self.batch_first:\n",
    "            # (t, b, c, h, w) -> (b, t, c, h, w)\n",
    "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
    "\n",
    "        b, _, _, h, w = input_tensor.size()\n",
    "\n",
    "        # Implement stateful ConvLSTM\n",
    "        if hidden_state is not None:\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            # Since the init is done in forward. Can send image size here\n",
    "            hidden_state = self._init_hidden(batch_size=b,\n",
    "                                             image_size=(h, w))\n",
    "\n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    "\n",
    "        seq_len = input_tensor.size(1)\n",
    "        cur_layer_input = input_tensor\n",
    "\n",
    "        for layer_idx in range(self.num_layers):\n",
    "\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            for t in range(seq_len):\n",
    "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],\n",
    "                                                 cur_state=[h, c])\n",
    "                output_inner.append(h)\n",
    "\n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            cur_layer_input = layer_output\n",
    "\n",
    "            layer_output_list.append(layer_output)\n",
    "            last_state_list.append([h, c])\n",
    "\n",
    "        if not self.return_all_layers:\n",
    "            layer_output_list = layer_output_list[-1:]\n",
    "            last_state_list = last_state_list[-1:]\n",
    "\n",
    "        return layer_output_list, last_state_list\n",
    "\n",
    "    def _init_hidden(self, batch_size, image_size):\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n",
    "        return init_states\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_kernel_size_consistency(kernel_size):\n",
    "        if not (isinstance(kernel_size, tuple) or\n",
    "                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
    "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
    "\n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers):\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7803a3f",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "79b0762f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T16:57:21.221280Z",
     "iopub.status.busy": "2024-12-17T16:57:21.220979Z",
     "iopub.status.idle": "2024-12-17T16:57:30.443001Z",
     "shell.execute_reply": "2024-12-17T16:57:30.442112Z"
    },
    "papermill": {
     "duration": 9.228972,
     "end_time": "2024-12-17T16:57:30.445690",
     "exception": false,
     "start_time": "2024-12-17T16:57:21.216718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "interpolated_weather_df = pd.read_csv('data/region/vietnam/interpolated_weather.csv', index_col=0, parse_dates=True)\n",
    "interpolated_air_df = pd.read_csv('data/region/vietnam/interpolated_air.csv', index_col=0, parse_dates=True)\n",
    "#interpolated_weather_df = pd.read_csv('/kaggle/input/air-and-weather/interpolated_weather.csv', index_col=0, parse_dates=True)\n",
    "#interpolated_air_df = pd.read_csv('/kaggle/input/air-and-weather/interpolated_air.csv', index_col=0, parse_dates=True)\n",
    "interpolated_air_df.drop(columns='aqi', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a14c7a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T16:57:30.460007Z",
     "iopub.status.busy": "2024-12-17T16:57:30.459185Z",
     "iopub.status.idle": "2024-12-17T16:57:30.644155Z",
     "shell.execute_reply": "2024-12-17T16:57:30.643202Z"
    },
    "papermill": {
     "duration": 0.194044,
     "end_time": "2024-12-17T16:57:30.646228",
     "exception": false,
     "start_time": "2024-12-17T16:57:30.452184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "interpolated_weather_df[\"wind_x_component\"] = np.cos(interpolated_weather_df[\"wind_direction_10m\"] / (180 / np.pi))\n",
    "interpolated_weather_df[\"wind_y_component\"] = np.sin(interpolated_weather_df[\"wind_direction_10m\"] / (180 / np.pi))\n",
    "interpolated_weather_df.drop(columns='wind_direction_10m', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1942c063",
   "metadata": {},
   "source": [
    "Train, validation, test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69392f3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T16:57:30.656185Z",
     "iopub.status.busy": "2024-12-17T16:57:30.655606Z",
     "iopub.status.idle": "2024-12-17T16:57:30.715524Z",
     "shell.execute_reply": "2024-12-17T16:57:30.714817Z"
    },
    "papermill": {
     "duration": 0.067651,
     "end_time": "2024-12-17T16:57:30.717548",
     "exception": false,
     "start_time": "2024-12-17T16:57:30.649897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "air_train, air_test_and_val = interpolated_air_df.loc[:'2023-12-31 23:00:00'], interpolated_air_df.loc['2024-01-01 00:00:00':]\n",
    "weather_train, weather_test_and_val = interpolated_weather_df.loc[:'2023-12-31 23:00:00'], interpolated_weather_df.loc['2024-01-01 00:00:00':]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "096879a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T16:57:30.725604Z",
     "iopub.status.busy": "2024-12-17T16:57:30.725311Z",
     "iopub.status.idle": "2024-12-17T16:57:30.730760Z",
     "shell.execute_reply": "2024-12-17T16:57:30.730179Z"
    },
    "papermill": {
     "duration": 0.01118,
     "end_time": "2024-12-17T16:57:30.732306",
     "exception": false,
     "start_time": "2024-12-17T16:57:30.721126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "indices = np.arange(len(air_test_and_val))\n",
    "half_way = int(len(indices) * 0.5)\n",
    "\n",
    "air_valid = air_test_and_val.iloc[:half_way]\n",
    "air_test = air_test_and_val.iloc[half_way:]\n",
    "\n",
    "weather_valid = weather_test_and_val.iloc[:half_way]\n",
    "weather_test = weather_test_and_val.iloc[half_way:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edb8bf9",
   "metadata": {},
   "source": [
    "Sort by province and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "67f74294",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T16:57:30.740145Z",
     "iopub.status.busy": "2024-12-17T16:57:30.739855Z",
     "iopub.status.idle": "2024-12-17T16:57:31.794772Z",
     "shell.execute_reply": "2024-12-17T16:57:31.794108Z"
    },
    "papermill": {
     "duration": 1.061148,
     "end_time": "2024-12-17T16:57:31.796812",
     "exception": false,
     "start_time": "2024-12-17T16:57:30.735664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "air_train = air_train.reset_index().sort_values(by=['province', 'time'])\n",
    "weather_train = weather_train.reset_index().sort_values(by=['province', 'time'])\n",
    "\n",
    "air_valid = air_valid.reset_index().sort_values(by=['province', 'time'])\n",
    "weather_valid = weather_valid.reset_index().sort_values(by=['province', 'time'])\n",
    "\n",
    "air_test = air_test.reset_index().sort_values(by=['province', 'time'])\n",
    "weather_test = weather_test.reset_index().sort_values(by=['province', 'time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "def6cc14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T16:57:31.804982Z",
     "iopub.status.busy": "2024-12-17T16:57:31.804674Z",
     "iopub.status.idle": "2024-12-17T16:57:31.901886Z",
     "shell.execute_reply": "2024-12-17T16:57:31.901104Z"
    },
    "papermill": {
     "duration": 0.103422,
     "end_time": "2024-12-17T16:57:31.903851",
     "exception": false,
     "start_time": "2024-12-17T16:57:31.800429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "air_train.drop(columns=['province', 'time'], inplace=True)\n",
    "weather_train.drop(columns=['province', 'time'], inplace=True)\n",
    "\n",
    "air_valid.drop(columns=['province', 'time'], inplace=True)\n",
    "weather_valid.drop(columns=['province', 'time'], inplace=True)\n",
    "\n",
    "air_test.drop(columns=['province', 'time'], inplace=True)\n",
    "weather_test.drop(columns=['province', 'time'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5430d808",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T16:57:31.912438Z",
     "iopub.status.busy": "2024-12-17T16:57:31.911824Z",
     "iopub.status.idle": "2024-12-17T16:57:31.920812Z",
     "shell.execute_reply": "2024-12-17T16:57:31.920077Z"
    },
    "papermill": {
     "duration": 0.014928,
     "end_time": "2024-12-17T16:57:31.922442",
     "exception": false,
     "start_time": "2024-12-17T16:57:31.907514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TimeSeries3DDataset(Dataset):\n",
    "    def __init__(self, target, features, n_provinces, sequence_length=3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            target (Dataframe): Input dataframe\n",
    "            features (Dataframe): Output dataframe\n",
    "            n_provinces (int): number of channels\n",
    "            sequence_length (int, optional): Time window size. Defaults to 3.\n",
    "        \"\"\"\n",
    "        self.target_scaler = preprocessing.StandardScaler()\n",
    "        self.features_scaler = preprocessing.StandardScaler()\n",
    "        self.features = self.features_scaler.fit_transform(features.values)\n",
    "        self.target = self.target_scaler.fit_transform(target.values)\n",
    "\n",
    "        self.X = torch.tensor(self.features.reshape(n_provinces, len(features)//n_provinces, len(features.columns))).float()\n",
    "        self.y = torch.tensor(self.target.reshape(n_provinces, len(target)//n_provinces, len(target.columns))).float()\n",
    "\n",
    "        self.sequence_length = sequence_length\n",
    "        self.features_length = self.features.shape[-1]\n",
    "        self.target_length = self.target.shape[-1]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[1]\n",
    "\n",
    "    def _mirror_padding(self, x, sequence_length, padding_needed):\n",
    "        mirrored_part = torch.flip(x[:, :padding_needed, :], dims=[1])\n",
    "        padded_x = torch.cat([mirrored_part, x], dim=1)\n",
    "        return padded_x\n",
    "\n",
    "    def _get_window(self, X, i):\n",
    "        \"\"\"\n",
    "        Get time window, mirror padding if needed\n",
    "        \"\"\"\n",
    "        if i >= self.sequence_length - 1:\n",
    "            i_start = i - self.sequence_length + 1\n",
    "            x = X[:, i_start:(i + 1), :]\n",
    "        else:\n",
    "            padding_needed = self.sequence_length - i - 1\n",
    "            x = self._mirror_padding(X, self.sequence_length, padding_needed)\n",
    "            x = x[:, :self.sequence_length, :]\n",
    "        return x\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x_window = self._get_window(self.X, i)\n",
    "        x_window = x_window.permute(1, 0, 2).unsqueeze(2)\n",
    "        return x_window, self.y[:, i, :].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ed86ce7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T16:57:31.936500Z",
     "iopub.status.busy": "2024-12-17T16:57:31.936247Z",
     "iopub.status.idle": "2024-12-17T16:57:31.943083Z",
     "shell.execute_reply": "2024-12-17T16:57:31.942322Z"
    },
    "papermill": {
     "duration": 0.012438,
     "end_time": "2024-12-17T16:57:31.944598",
     "exception": false,
     "start_time": "2024-12-17T16:57:31.932160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvLSTMTimeSeries(nn.Module):\n",
    "    \"\"\"\n",
    "    ConvLSTM with Linear Layer as final layer\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, input_width, output_width):\n",
    "        super().__init__()\n",
    "        self.conv_lstm = ConvLSTM(\n",
    "            input_dim = input_dim,\n",
    "            hidden_dim = hidden_dim,\n",
    "            kernel_size = (1 , input_width),\n",
    "            num_layers = len(hidden_dim),\n",
    "            batch_first = True\n",
    "            )\n",
    "        self.linear = nn.Linear(hidden_dim[-1] * input_width, input_dim*output_width)\n",
    "        self.flatten = nn.Flatten(1, -1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        _, last_states = self.conv_lstm(X)\n",
    "        X = last_states[0][0]\n",
    "        X = self.flatten(X)\n",
    "        X = self.linear(X)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def predict(self, X, numpy_output=True):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            output = self(X)\n",
    "        if numpy_output:\n",
    "            output = output.numpy()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a741ae8",
   "metadata": {},
   "source": [
    "Bayes Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a0a9a71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T16:57:31.952407Z",
     "iopub.status.busy": "2024-12-17T16:57:31.951831Z",
     "iopub.status.idle": "2024-12-17T16:57:32.209654Z",
     "shell.execute_reply": "2024-12-17T16:57:32.208757Z"
    },
    "papermill": {
     "duration": 0.263931,
     "end_time": "2024-12-17T16:57:32.211752",
     "exception": false,
     "start_time": "2024-12-17T16:57:31.947821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\n",
    "        \"name\": \"val_loss\",\n",
    "        \"goal\": \"minimize\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"lr\": {\n",
    "            \"values\": [1e-3, 1e-4]\n",
    "        },\n",
    "        \"weight_decay\": {\n",
    "            \"values\": [0, 1e-5, 1e-3]\n",
    "        },\n",
    "        \"patience\": {\n",
    "            \"values\": [3, 5, 10]\n",
    "        },\n",
    "        \"hidden_dim\": {\n",
    "            \"values\": [[128], [128, 64], [256, 128, 64], [128, 64, 32]]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "#sweep_id = wandb.sweep(sweep_config, project=\"data_science\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0ea6484",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T16:57:32.219828Z",
     "iopub.status.busy": "2024-12-17T16:57:32.219565Z",
     "iopub.status.idle": "2024-12-17T16:57:32.913764Z",
     "shell.execute_reply": "2024-12-17T16:57:32.912819Z"
    },
    "papermill": {
     "duration": 0.700679,
     "end_time": "2024-12-17T16:57:32.915944",
     "exception": false,
     "start_time": "2024-12-17T16:57:32.215265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_provinces = 63\n",
    "window_size = 3\n",
    "\n",
    "train_dataset = TimeSeries3DDataset(air_train, weather_train, n_provinces, window_size)\n",
    "valid_dataset = TimeSeries3DDataset(air_valid, weather_valid, n_provinces, window_size)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers = 0, pin_memory=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=32, shuffle=False, num_workers = 0, pin_memory=True)\n",
    "\n",
    "model = ConvLSTMTimeSeries(\n",
    "    input_dim = n_provinces,\n",
    "    hidden_dim = [256, 128, 64],\n",
    "    input_width = train_dataset.features_length,\n",
    "    output_width = train_dataset.target_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2337aeb",
   "metadata": {},
   "source": [
    "Save scaler of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4bf4939b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/air_scaler.pickle']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(train_dataset.features_scaler, 'models/weather_scaler.pickle')\n",
    "joblib.dump(train_dataset.target_scaler, 'models/air_scaler.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192e6813",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9e06275",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T16:57:32.924319Z",
     "iopub.status.busy": "2024-12-17T16:57:32.924028Z",
     "iopub.status.idle": "2024-12-17T16:57:32.996382Z",
     "shell.execute_reply": "2024-12-17T16:57:32.995461Z"
    },
    "papermill": {
     "duration": 0.078688,
     "end_time": "2024-12-17T16:57:32.998365",
     "exception": false,
     "start_time": "2024-12-17T16:57:32.919677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "\n",
    "        model = ConvLSTMTimeSeries(\n",
    "            input_dim = n_provinces,\n",
    "            hidden_dim = config.hidden_dim,\n",
    "            input_width = train_dataset.features_length,\n",
    "            output_width = train_dataset.target_length\n",
    "            )\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=config.patience)\n",
    "        criterion = nn.MSELoss()\n",
    "        num_epochs = 50\n",
    "        val_patience = 20\n",
    "        waited_epoch = 0\n",
    "        best_val_loss = float('inf')\n",
    "\n",
    "        model.to(device)\n",
    "        model.train()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "            print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "            train_loader = tqdm(train_dataloader, desc=\"Training\", leave=False)\n",
    "            for X, y in train_loader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output = model(X)\n",
    "                loss = criterion(output, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                train_loader.set_postfix({\"Loss\": f\"{total_loss / (train_loader.n + 1):.4f}\"})\n",
    "            \n",
    "            avg_train_loss = total_loss / len(train_dataloader)\n",
    "            print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "            model.eval()\n",
    "            total_val_loss = 0\n",
    "            valid_loader = tqdm(valid_dataloader, desc=\"Validation\", leave=False)\n",
    "            with torch.no_grad():\n",
    "                for X, y in valid_loader:\n",
    "                    X, y = X.to(device), y.to(device)\n",
    "                    output = model(X)\n",
    "                    loss = criterion(output, y)\n",
    "                    total_val_loss += loss.item()\n",
    "                    valid_loader.set_postfix({\"Val Loss\": f\"{total_val_loss / (valid_loader.n + 1):.4f}\"})\n",
    "\n",
    "            avg_val_loss = total_val_loss / len(valid_dataloader)\n",
    "            print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "            scheduler.step(avg_val_loss)\n",
    "            wandb.log({\"epoch\": epoch, \"train_loss\": avg_train_loss, \"val_loss\": avg_val_loss})\n",
    "\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                waited_epoch = 0\n",
    "                best_val_loss = avg_val_loss\n",
    "                torch.save(model.state_dict(), \"conv_lstm.pth\")\n",
    "            else:\n",
    "                waited_epoch += 1\n",
    "                if waited_epoch >= val_patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22d74aaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T16:57:33.009859Z",
     "iopub.status.busy": "2024-12-17T16:57:33.009268Z",
     "iopub.status.idle": "2024-12-17T20:01:43.331111Z",
     "shell.execute_reply": "2024-12-17T20:01:43.330453Z"
    },
    "papermill": {
     "duration": 11050.329118,
     "end_time": "2024-12-17T20:01:43.332719",
     "exception": false,
     "start_time": "2024-12-17T16:57:33.003601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#wandb.agent(sweep_id, train, count=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06e439a",
   "metadata": {},
   "source": [
    "Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c818704",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TimeSeries3DDataset(air_test, weather_test, n_provinces, window_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers = 0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7fb482b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_conv_lstm = ConvLSTMTimeSeries(\n",
    "    input_dim = n_provinces,\n",
    "    hidden_dim = [256],\n",
    "    input_width = train_dataset.features_length,\n",
    "    output_width = train_dataset.target_length\n",
    ")\n",
    "best_conv_lstm.load_state_dict(torch.load(\"models/conv_lstm.pth\", map_location=\"cpu\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bdc15dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = []\n",
    "    true_values = []\n",
    "    for X, y in test_dataloader:\n",
    "        output = best_conv_lstm.predict(X, numpy_output=False).squeeze()\n",
    "        output = output.view(n_provinces, 1, test_dataset.target_length)\n",
    "        outputs.append(output)\n",
    "\n",
    "        y = y.squeeze()\n",
    "        y = y.view(n_provinces, 1, test_dataset.target_length)\n",
    "        true_values.append(y)\n",
    "\n",
    "    stacked_outputs = torch.cat(outputs, dim=1)\n",
    "    original_outputs = stacked_outputs.view(-1, 6)\n",
    "    original_outputs = train_dataset.target_scaler.inverse_transform(original_outputs)\n",
    "\n",
    "    stacked_true = torch.cat(true_values, dim=1)\n",
    "    original_true = stacked_true.view(-1, 6)\n",
    "    original_true = train_dataset.target_scaler.inverse_transform(original_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ad6329c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d84acf",
   "metadata": {},
   "source": [
    "Root Mean Squared Error and Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4b4e76bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "co       328.524765\n",
       "no2        7.565302\n",
       "o3        21.680334\n",
       "so2        5.373150\n",
       "pm2_5     29.756366\n",
       "pm10      32.302020\n",
       "dtype: float64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(root_mean_squared_error(original_outputs, original_true, multioutput=\"raw_values\"), \n",
    "          index=[\"co\", \"no2\", \"o3\", \"so2\", \"pm2_5\", \"pm10\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6e0b1767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "co       187.566911\n",
       "no2        4.381024\n",
       "o3        14.947572\n",
       "so2        2.730824\n",
       "pm2_5     16.620971\n",
       "pm10      18.238303\n",
       "dtype: float64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(mean_absolute_error(original_outputs, original_true, multioutput=\"raw_values\"), \n",
    "          index=[\"co\", \"no2\", \"o3\", \"so2\", \"pm2_5\", \"pm10\"])"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6323892,
     "sourceId": 10228367,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11121.403471,
   "end_time": "2024-12-17T20:02:33.630630",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-17T16:57:12.227159",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
