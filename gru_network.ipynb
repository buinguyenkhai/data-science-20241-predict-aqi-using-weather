{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from joblib import dump, load\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error\n",
    "from ds_code.function.utils import sliding_window\n",
    "from ds_code.function.models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extra attributes for distinction between provinces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>population</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1704774326</th>\n",
       "      <td>10.7756</td>\n",
       "      <td>106.7019</td>\n",
       "      <td>1.513600e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704413791</th>\n",
       "      <td>21.0000</td>\n",
       "      <td>105.8500</td>\n",
       "      <td>8.246600e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704000623</th>\n",
       "      <td>20.8651</td>\n",
       "      <td>106.6838</td>\n",
       "      <td>2.103500e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704783472</th>\n",
       "      <td>10.0333</td>\n",
       "      <td>105.7833</td>\n",
       "      <td>1.237300e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704863046</th>\n",
       "      <td>10.9500</td>\n",
       "      <td>106.8167</td>\n",
       "      <td>1.104000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704495953</th>\n",
       "      <td>22.8333</td>\n",
       "      <td>104.9833</td>\n",
       "      <td>5.555900e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704000217</th>\n",
       "      <td>22.1333</td>\n",
       "      <td>105.8333</td>\n",
       "      <td>4.503600e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704983526</th>\n",
       "      <td>22.3992</td>\n",
       "      <td>103.4392</td>\n",
       "      <td>4.297300e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704988146</th>\n",
       "      <td>14.3544</td>\n",
       "      <td>108.0075</td>\n",
       "      <td>6.586050e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704949870</th>\n",
       "      <td>16.0748</td>\n",
       "      <td>108.2240</td>\n",
       "      <td>6.586050e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                lat       lng    population\n",
       "id                                         \n",
       "1704774326  10.7756  106.7019  1.513600e+07\n",
       "1704413791  21.0000  105.8500  8.246600e+06\n",
       "1704000623  20.8651  106.6838  2.103500e+06\n",
       "1704783472  10.0333  105.7833  1.237300e+06\n",
       "1704863046  10.9500  106.8167  1.104000e+06\n",
       "...             ...       ...           ...\n",
       "1704495953  22.8333  104.9833  5.555900e+04\n",
       "1704000217  22.1333  105.8333  4.503600e+04\n",
       "1704983526  22.3992  103.4392  4.297300e+04\n",
       "1704988146  14.3544  108.0075  6.586050e+05\n",
       "1704949870  16.0748  108.2240  6.586050e+05\n",
       "\n",
       "[63 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_data = pd.read_csv(\"data/region/vietnam/extra_info.csv\", index_col=0)\n",
    "city_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_np = []\n",
    "air_np = []\n",
    "init_np = []\n",
    "\n",
    "# take some minutes to run\n",
    "for city_id in city_data.index:        \n",
    "    # load air quality and weather data files \n",
    "    air_df = pd.read_csv(\"data/air_quality/\" + str(city_id) + \".csv\")\n",
    "    weather_df = pd.read_csv(\"data/weather/\" + str(city_id) + \".csv\")\n",
    "    \n",
    "    # air quality data preprocessing\n",
    "    air_df = air_df.loc[(air_df.iloc[:, 1:] >= 0).all(axis=1)]\n",
    "    air_df.drop(\"aqi\", axis=1, inplace=True)\n",
    "    air_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # weather data preprocessing\n",
    "    weather_df.dropna(axis=0, inplace=True)\n",
    "    weather_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # convert wind direction attribute\n",
    "    weather_df[\"wind_x_component\"] = np.cos(weather_df[\"wind_direction_10m\"] / (180 / np.pi))\n",
    "    weather_df[\"wind_y_component\"] = np.sin(weather_df[\"wind_direction_10m\"] / (180 / np.pi))\n",
    "    weather_df.drop(\"wind_direction_10m\", axis=1, inplace=True)\n",
    "    \n",
    "    # making sliding windows\n",
    "    X, y = sliding_window(weather_df, air_df, target_size=\"same\")\n",
    "    \n",
    "    # extra attibutes\n",
    "    m = X.shape[0]\n",
    "    extra_attr = city_data.loc[city_id]\n",
    "    lat = np.full((m, 1), extra_attr[0])\n",
    "    lng = np.full((m, 1), extra_attr[1])\n",
    "    population = np.full((m, 1), extra_attr[2])\n",
    "    init_data = np.hstack((lat, lng, population))\n",
    "    \n",
    "    weather_np.append(X)\n",
    "    air_np.append(y)\n",
    "    init_np.append(init_data)\n",
    "    \n",
    "weather_np = np.vstack(weather_np)\n",
    "air_np = np.vstack(air_np)\n",
    "init_np = np.vstack(init_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gru_raw_dataset.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dump((weather_np, air_np, init_np), \"gru_raw_dataset.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weather_np, air_np, init_np = load(\"gru_raw_dataset.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scaler for normalizing and revert data to original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardScaler(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StandardScaler, self).__init__()\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.mean = X.mean(dim=0, keepdim=True)\n",
    "        self.std = X.std(dim=0, keepdim=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return (X - self.mean) / self.std\n",
    "\n",
    "    def inverse_transform(self, X_scaled):\n",
    "        return X_scaled * self.std + self.mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGRU(nn.Module):\n",
    "    def __init__(self, input_size, output_size, seq_len=4, label_scaler=None):\n",
    "        super(CustomGRU, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.seq_len = seq_len\n",
    "        self.label_scaler = label_scaler\n",
    "        \n",
    "        # fully connected layers to generate initial hidden state for GRU layers\n",
    "        self.init_nn = nn.Sequential(\n",
    "            nn.LayerNorm(3),\n",
    "            nn.Linear(3, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # GRU layers\n",
    "        self.flatten = nn.Flatten(1, -1)\n",
    "        self.normalize = nn.LayerNorm(input_size * seq_len)\n",
    "        self.gru1 = nn.GRU(input_size, 256, batch_first=True)\n",
    "        self.gru2 = nn.GRU(256, 128, batch_first=True)\n",
    "        self.gru3 = nn.GRU(128, 64, batch_first=True)\n",
    "        \n",
    "        # Final fully connected layer\n",
    "        self.linear = nn.Linear(64, output_size)\n",
    "\n",
    "    def forward(self, inp, rescale=False):\n",
    "        X, init_data = inp\n",
    "        X = self.flatten(X)\n",
    "        X = self.normalize(X).reshape((-1, self.seq_len, self.input_size))\n",
    "        init_data = self.init_nn(init_data.unsqueeze(0))\n",
    "        X, _ = self.gru1(X, init_data)\n",
    "        X, _ = self.gru2(X)\n",
    "        X, _ = self.gru3(X)\n",
    "        X = self.linear(X)\n",
    "        # Rescale if needed with a standard scaler (for actual prediction)\n",
    "        if rescale:\n",
    "            X = self.label_scaler.inverse_transform(X)\n",
    "        return X\n",
    "    \n",
    "    def predict(self, inp, numpy_output=True):\n",
    "        X, init_data = inp\n",
    "        inp = (Tensor(X), Tensor(init_data))\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            output = self(inp, rescale=True)\n",
    "        if numpy_output:\n",
    "            output = output.numpy()\n",
    "        return output[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y, init_data, label_scaler=None):\n",
    "        self.X = Tensor(X)\n",
    "        self.y = Tensor(y)\n",
    "        self.init_data = Tensor(init_data)\n",
    "        \n",
    "        if label_scaler is not None:\n",
    "            self.scaler = label_scaler\n",
    "            self.scaler.fit(self.y)\n",
    "            self.y = label_scaler(self.y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (self.X[index], self.init_data[index]), self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, valid_dataloader, learning_rate=1e-3, num_epochs=10):\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=0)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, patience=5)\n",
    "    criterion = nn.MSELoss()\n",
    "    val_patience = 20\n",
    "    waited_epoch = 0\n",
    "    best_val_loss = 9999\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "        for i, (X, y) in enumerate(train_dataloader):\n",
    "            X = X[0].to(device), X[1].to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            output = model(X)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(output, y)\n",
    "            total_loss += loss\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (i + 1) % 1000 == 0:\n",
    "                print(f\"Batch {i + 1}:\", f\"Loss: {total_loss / i:.4f}\")\n",
    "\n",
    "        print(f\"Training loss: {total_loss / len(train_dataloader):.4f}\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            total_loss = 0\n",
    "            for X, y in valid_dataloader:\n",
    "                X = X[0].to(device), X[1].to(device)\n",
    "                y = y.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                output = model(X)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = criterion(output[:, -1], y[:, -1])\n",
    "                total_loss += loss\n",
    "            scheduler.step(total_loss)\n",
    "            \n",
    "        print(f\"Validaton loss: {total_loss / len(valid_dataloader):.4f}\\n\")\n",
    "            \n",
    "        if total_loss < best_val_loss:\n",
    "            waited_epoch = 0\n",
    "            best_val_loss = total_loss\n",
    "            torch.save(model, \"models/gru.pth\")\n",
    "        else:\n",
    "            waited_epoch += 1\n",
    "            if waited_epoch == val_patience:\n",
    "                print(\"Training stopped.\")\n",
    "                return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reduce data size for memory efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_np = weather_np.astype(\"float32\")\n",
    "air_np = air_np.astype(\"float32\")\n",
    "init_np = init_np.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "idx = [i for i in range(len(weather_np))]\n",
    "random.shuffle(idx)\n",
    "train_idx, test_idx = idx[:1800000], idx[1800000:]\n",
    "X_train, X_test, y_train, y_test, init_train, init_test = weather_np[train_idx], weather_np[test_idx], air_np[train_idx], air_np[test_idx], init_np[train_idx], init_np[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare training and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaler = StandardScaler()\n",
    "valid_scaler = StandardScaler()\n",
    "train_dataset = TimeSeriesDataset(X_train[:1700000], y_train[:1700000], init_train[:1700000], train_scaler)\n",
    "valid_dataset = TimeSeriesDataset(X_train[1700000:], y_train[1700000:], init_train[1700000:], valid_scaler)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomGRU(\n",
       "  (label_scaler): StandardScaler()\n",
       "  (init_nn): Sequential(\n",
       "    (0): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=3, out_features=128, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (4): ReLU()\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (normalize): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "  (gru1): GRU(9, 256, batch_first=True)\n",
       "  (gru2): GRU(256, 128, batch_first=True)\n",
       "  (gru3): GRU(128, 64, batch_first=True)\n",
       "  (linear): Linear(in_features=64, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = CustomGRU(input_size=9, output_size=6, seq_len=4, label_scaler=train_scaler)\n",
    "rnn.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "Learning rate: 0.001\n",
      "Training loss: 1.0024\n",
      "Validaton loss: 0.9818\n",
      "\n",
      "Epoch 2/200\n",
      "Learning rate: 0.001\n",
      "Training loss: 0.9957\n",
      "Validaton loss: 0.9784\n",
      "\n",
      "Epoch 3/200\n",
      "Learning rate: 0.001\n",
      "Training loss: 1.0086\n",
      "Validaton loss: 0.9665\n",
      "\n",
      "Epoch 4/200\n",
      "Learning rate: 0.001\n",
      "Training loss: 0.9717\n",
      "Validaton loss: 0.9831\n",
      "\n",
      "Epoch 5/200\n",
      "Learning rate: 0.001\n",
      "Training loss: 0.9665\n",
      "Validaton loss: 0.9327\n",
      "\n",
      "Epoch 6/200\n",
      "Learning rate: 0.001\n",
      "Training loss: 0.9364\n",
      "Validaton loss: 0.9256\n",
      "\n",
      "Epoch 7/200\n",
      "Learning rate: 0.001\n",
      "Training loss: 0.9351\n",
      "Validaton loss: 0.9331\n",
      "\n",
      "Epoch 8/200\n",
      "Learning rate: 0.001\n",
      "Training loss: 0.9300\n",
      "Validaton loss: 0.9721\n",
      "\n",
      "Epoch 9/200\n",
      "Learning rate: 0.001\n",
      "Training loss: 0.9270\n",
      "Validaton loss: 0.9240\n",
      "\n",
      "Epoch 10/200\n",
      "Learning rate: 0.001\n",
      "Training loss: 0.9205\n",
      "Validaton loss: 0.9169\n",
      "\n",
      "Epoch 11/200\n",
      "Learning rate: 0.001\n",
      "Training loss: 0.9220\n",
      "Validaton loss: 0.9308\n",
      "\n",
      "Epoch 12/200\n",
      "Learning rate: 0.001\n",
      "Training loss: 0.9122\n",
      "Validaton loss: 0.9427\n",
      "\n",
      "Epoch 13/200\n",
      "Learning rate: 0.001\n",
      "Training loss: 0.9154\n",
      "Validaton loss: 0.9136\n",
      "\n",
      "Epoch 14/200\n",
      "Learning rate: 0.001\n",
      "Training loss: 0.9146\n",
      "Validaton loss: 1.0889\n",
      "\n",
      "Epoch 15/200\n",
      "Learning rate: 0.001\n",
      "Training loss: 0.9030\n",
      "Validaton loss: 0.9560\n",
      "\n",
      "Epoch 16/200\n",
      "Learning rate: 0.001\n",
      "Training loss: 0.9083\n",
      "Validaton loss: 0.9175\n",
      "\n",
      "Epoch 17/200\n",
      "Learning rate: 0.001\n",
      "Training loss: 0.8958\n",
      "Validaton loss: 0.8939\n",
      "\n",
      "Epoch 18/200\n",
      "Learning rate: 0.001\n",
      "Training loss: 0.8977\n",
      "Validaton loss: 0.9874\n",
      "\n",
      "Epoch 19/200\n",
      "Learning rate: 0.001\n",
      "Training loss: 0.9001\n",
      "Validaton loss: 0.9046\n",
      "\n",
      "Epoch 20/200\n",
      "Learning rate: 0.001\n",
      "Training loss: 0.8878\n",
      "Validaton loss: 0.8769\n",
      "\n",
      "Epoch 21/200\n",
      "Learning rate: 0.001\n",
      "Training loss: 0.8821\n",
      "Validaton loss: 0.9204\n",
      "\n",
      "Epoch 22/200\n",
      "Learning rate: 0.001\n",
      "Training loss: 0.9141\n",
      "Validaton loss: 0.9844\n",
      "\n",
      "Epoch 23/200\n",
      "Learning rate: 0.001\n",
      "Training loss: 0.8871\n",
      "Validaton loss: 0.8786\n",
      "\n",
      "Epoch 24/200\n",
      "Learning rate: 0.001\n",
      "Training loss: 0.9016\n",
      "Validaton loss: 0.9791\n",
      "\n",
      "Epoch 25/200\n",
      "Learning rate: 0.001\n",
      "Training loss: 0.8854\n",
      "Validaton loss: 0.8623\n",
      "\n",
      "Epoch 26/200\n",
      "Learning rate: 0.001\n",
      "Training loss: 0.8944\n",
      "Validaton loss: 0.8516\n",
      "\n",
      "Epoch 27/200\n",
      "Learning rate: 0.001\n",
      "Training loss: 0.8956\n",
      "Validaton loss: 1.0748\n",
      "\n",
      "Epoch 28/200\n",
      "Learning rate: 0.001\n",
      "Training loss: 0.8903\n",
      "Validaton loss: 0.9025\n",
      "\n",
      "Epoch 29/200\n",
      "Learning rate: 0.001\n",
      "Training loss: 0.8675\n",
      "Validaton loss: 0.9029\n",
      "\n",
      "Epoch 30/200\n",
      "Learning rate: 0.001\n",
      "Training loss: 0.8765\n",
      "Validaton loss: 0.9167\n",
      "\n",
      "Epoch 31/200\n",
      "Learning rate: 0.001\n",
      "Training loss: 0.8792\n",
      "Validaton loss: 0.8937\n",
      "\n",
      "Epoch 32/200\n",
      "Learning rate: 0.001\n",
      "Training loss: 0.8655\n",
      "Validaton loss: 0.8814\n",
      "\n",
      "Epoch 33/200\n",
      "Learning rate: 0.0001\n",
      "Training loss: 0.8571\n",
      "Validaton loss: 0.8725\n",
      "\n",
      "Epoch 34/200\n",
      "Learning rate: 0.0001\n",
      "Training loss: 0.8588\n",
      "Validaton loss: 0.8505\n",
      "\n",
      "Epoch 35/200\n",
      "Learning rate: 0.0001\n",
      "Training loss: 0.8541\n",
      "Validaton loss: 0.9167\n",
      "\n",
      "Epoch 36/200\n",
      "Learning rate: 0.0001\n",
      "Training loss: 0.8555\n",
      "Validaton loss: 0.8519\n",
      "\n",
      "Epoch 37/200\n",
      "Learning rate: 0.0001\n",
      "Training loss: 0.8533\n",
      "Validaton loss: 0.8422\n",
      "\n",
      "Epoch 38/200\n",
      "Learning rate: 0.0001\n",
      "Training loss: 0.8546\n",
      "Validaton loss: 0.8469\n",
      "\n",
      "Epoch 39/200\n",
      "Learning rate: 0.0001\n",
      "Training loss: 0.8597\n",
      "Validaton loss: 0.8686\n",
      "\n",
      "Epoch 40/200\n",
      "Learning rate: 0.0001\n",
      "Training loss: 0.8688\n",
      "Validaton loss: 0.8480\n",
      "\n",
      "Epoch 41/200\n",
      "Learning rate: 0.0001\n",
      "Training loss: 0.8576\n",
      "Validaton loss: 0.8551\n",
      "\n",
      "Epoch 42/200\n",
      "Learning rate: 0.0001\n",
      "Training loss: 0.8562\n",
      "Validaton loss: 0.8584\n",
      "\n",
      "Epoch 43/200\n",
      "Learning rate: 0.0001\n",
      "Training loss: 0.8517\n",
      "Validaton loss: 0.8557\n",
      "\n",
      "Epoch 44/200\n",
      "Learning rate: 1e-05\n",
      "Training loss: 0.8524\n",
      "Validaton loss: 0.8421\n",
      "\n",
      "Epoch 45/200\n",
      "Learning rate: 1e-05\n",
      "Training loss: 0.8492\n",
      "Validaton loss: 0.8438\n",
      "\n",
      "Epoch 46/200\n",
      "Learning rate: 1e-05\n",
      "Training loss: 0.8590\n",
      "Validaton loss: 0.8641\n",
      "\n",
      "Epoch 47/200\n",
      "Learning rate: 1e-05\n",
      "Training loss: 0.8526\n",
      "Validaton loss: 0.8427\n",
      "\n",
      "Epoch 48/200\n",
      "Learning rate: 1e-05\n",
      "Training loss: 0.8516\n",
      "Validaton loss: 0.8407\n",
      "\n",
      "Epoch 49/200\n",
      "Learning rate: 1e-05\n",
      "Training loss: 0.8503\n",
      "Validaton loss: 0.8483\n",
      "\n",
      "Epoch 50/200\n",
      "Learning rate: 1e-05\n",
      "Training loss: 0.8484\n",
      "Validaton loss: 0.8416\n",
      "\n",
      "Epoch 51/200\n",
      "Learning rate: 1e-05\n",
      "Training loss: 0.8549\n",
      "Validaton loss: 0.8429\n",
      "\n",
      "Epoch 52/200\n",
      "Learning rate: 1e-05\n",
      "Training loss: 0.8528\n",
      "Validaton loss: 0.8437\n",
      "\n",
      "Epoch 53/200\n",
      "Learning rate: 1e-05\n",
      "Training loss: 0.8524\n",
      "Validaton loss: 0.8384\n",
      "\n",
      "Epoch 54/200\n",
      "Learning rate: 1e-05\n",
      "Training loss: 0.8573\n",
      "Validaton loss: 0.8424\n",
      "\n",
      "Epoch 55/200\n",
      "Learning rate: 1e-05\n",
      "Training loss: 0.8643\n",
      "Validaton loss: 0.8460\n",
      "\n",
      "Epoch 56/200\n",
      "Learning rate: 1e-05\n",
      "Training loss: 0.8614\n",
      "Validaton loss: 0.8371\n",
      "\n",
      "Epoch 57/200\n",
      "Learning rate: 1e-05\n",
      "Training loss: 0.8552\n",
      "Validaton loss: 0.8422\n",
      "\n",
      "Epoch 58/200\n",
      "Learning rate: 1e-05\n",
      "Training loss: 0.8549\n",
      "Validaton loss: 0.8799\n",
      "\n",
      "Epoch 59/200\n",
      "Learning rate: 1e-05\n",
      "Training loss: 0.8572\n",
      "Validaton loss: 0.8472\n",
      "\n",
      "Epoch 60/200\n",
      "Learning rate: 1e-05\n",
      "Training loss: 0.8554\n",
      "Validaton loss: 0.9291\n",
      "\n",
      "Epoch 61/200\n",
      "Learning rate: 1e-05\n",
      "Training loss: 0.8520\n",
      "Validaton loss: 0.8409\n",
      "\n",
      "Epoch 62/200\n",
      "Learning rate: 1e-05\n",
      "Training loss: 0.8575\n",
      "Validaton loss: 0.9107\n",
      "\n",
      "Epoch 63/200\n",
      "Learning rate: 1.0000000000000002e-06\n",
      "Training loss: 0.8490\n",
      "Validaton loss: 0.8472\n",
      "\n",
      "Epoch 64/200\n",
      "Learning rate: 1.0000000000000002e-06\n",
      "Training loss: 0.8512\n",
      "Validaton loss: 0.8582\n",
      "\n",
      "Epoch 65/200\n",
      "Learning rate: 1.0000000000000002e-06\n",
      "Training loss: 0.8512\n",
      "Validaton loss: 0.8476\n",
      "\n",
      "Epoch 66/200\n",
      "Learning rate: 1.0000000000000002e-06\n",
      "Training loss: 0.8503\n",
      "Validaton loss: 0.8430\n",
      "\n",
      "Training stopped.\n"
     ]
    }
   ],
   "source": [
    "# output is for illustraion purpose\n",
    "train_model(rnn, train_dataloader, valid_dataloader, learning_rate=1e-3, num_epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trann\\AppData\\Local\\Temp\\ipykernel_25456\\2437652950.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(\"models/gru.pth\", map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"models/gru.pth\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Root mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "co       467.083984\n",
       "no2       11.538258\n",
       "o3        24.473648\n",
       "so2        8.739453\n",
       "pm2_5     34.836983\n",
       "pm10      39.177345\n",
       "dtype: float32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(root_mean_squared_error(model.predict((X_test, init_test)), y_test[:, -1], multioutput=\"raw_values\"), \n",
    "          index=[\"co\", \"no2\", \"o3\", \"so2\", \"pm2_5\", \"pm10\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mean absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "co       253.260666\n",
       "no2        7.176695\n",
       "o3        17.367544\n",
       "so2        4.860417\n",
       "pm2_5     20.831501\n",
       "pm10      23.449749\n",
       "dtype: float32"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(mean_absolute_error(model.predict((X_test, init_test)), y_test[:, -1], multioutput=\"raw_values\"), \n",
    "          index=[\"co\", \"no2\", \"o3\", \"so2\", \"pm2_5\", \"pm10\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
